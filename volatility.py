#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Volatility Calculator + Orderbook Percentile Depth Tables + Leverage Information + Leverage vs USD Chart + Time Series Charts

Analyzes 5-minute crypto snapshots to compute:
1) Volatility metrics (scaled from 5-min returns)
2) Leverage information from pre-saved file (generated by API.py)
3) Percentile orderbook depth tables using absolute spread distances
4) Column chart: Effective Leverage vs Median USD Quantity
5) Time series charts: Token price levels over time
6) Time series charts: USD price levels over time

Usage:
python volatility.py data.csv                              # Default percentiles [1, 10, 25, 50]
python volatility.py data.csv BTC ETH                      # Subset of symbols
python volatility.py data.csv --percentiles 5 25 75       # Custom percentiles
python volatility.py data.csv --sep ";"                   # Custom CSV separator
python volatility.py data.csv --chart-level 1             # Use specific orderbook level for chart (default: 1)

Leverage Data:
- Leverage data is read from leverage_data.json (generated by API.py)
- To update leverage data, run: python API.py BTC ETH SOL [symbols...]
- No API calls are made from this script - all data comes from the file
"""

import sys
import argparse
from pathlib import Path
from typing import Optional, Dict, Tuple, List
import numpy as np
import pandas as pd
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns

sys.path.append("/home/eric/Documents/growi")
from orderbook_utils import parse_orderbook_blob, validate_required_columns, REQUIRED_CSV_COLUMNS, _normalize_number
from API import get_leverage_info_from_file, LEVERAGE_DATA_FILE

DEFAULT_PERCENTILES = [1, 10, 25, 50]
EXCEL_FILENAME = "volatility_analysis.xlsx"
CHART_FILENAME = "leverage_vs_usd_chart.png"

def calculate_log_returns(prices: pd.Series) -> pd.Series:
    prices = prices[prices > 0]
    log_returns = np.log(prices / prices.shift(1))
    return log_returns.dropna()

def calculate_volatility_metrics(log_returns: pd.Series) -> Dict[str, float]:
    if len(log_returns) < 2:
        return {'3min_vol': 0.0, '5min_vol': 0.0, '10min_vol': 0.0, '30min_vol': 0.0,
                '60min_vol': 0.0, '90min_vol': 0.0, '1day_vol': 0.0, '1year_vol': 0.0,
                'sample_size': len(log_returns), 'mean_return': 0.0}
    vol_5min = log_returns.std()
    return {
        '3min_vol': vol_5min * np.sqrt(3/5), '5min_vol': vol_5min, '10min_vol': vol_5min * np.sqrt(10/5),
        '30min_vol': vol_5min * np.sqrt(30/5), '60min_vol': vol_5min * np.sqrt(60/5),
        '90min_vol': vol_5min * np.sqrt(90/5), '1day_vol': vol_5min * np.sqrt(1440/5),
        '1year_vol': vol_5min * np.sqrt(525600/5), 'sample_size': len(log_returns),
        'mean_return': log_returns.mean()
    }

def fetch_leverage_info(symbol: str) -> Dict:
    """
    Fetch leverage info from pre-saved file instead of making API calls.
    
    Args:
        symbol: Symbol to get leverage info for
        
    Returns:
        Leverage info dict
    """
    return get_leverage_info_from_file(symbol, LEVERAGE_DATA_FILE)

def get_effective_leverage_for_usd_amount(usd_amount: float, leverage_tiers: List[Tuple[float, float, int]]) -> int:
    """
    Given a USD amount and leverage tiers, return the maximum leverage available for that amount.
    
    leverage_tiers format: [(lower_bound, upper_bound, leverage), ...]
    Example: [(0.0, 20000000.0, 10), (20000000.0, inf, 5)]
    """
    if not leverage_tiers:
        return 0
    
    for lower_bound, upper_bound, leverage in leverage_tiers:
        if lower_bound <= usd_amount < upper_bound:
            return leverage
    
    # If no tier matches, return the leverage of the highest tier
    return leverage_tiers[-1][2] if leverage_tiers else 0

def extract_median_usd_from_p50(results: Dict, percentiles: List[int], orderbook_level: int = 1) -> Dict[str, Dict]:
    """
    Extract median USD quantities from p50 percentile data for each symbol.
    
    Args:
        results: Analysis results
        percentiles: List of percentiles (must include 50)
        orderbook_level: Which orderbook level to use (1 = best bid/ask, 2 = second level, etc.)
    
    Returns:
        Dict with symbol -> {
            'bid_usd': float,
            'ask_usd': float, 
            'avg_usd': float,
            'effective_leverage_bid': int,
            'effective_leverage_ask': int,
            'effective_leverage_avg': int
        }
    """
    if 50 not in percentiles:
        raise ValueError("Percentile 50 must be included to extract median USD data")
    
    # Build percentile data
    percentile_dfs = build_percentile_depth(results, percentiles)
    p50_df = percentile_dfs[50]
    
    leverage_usd_data = {}
    
    for symbol in results.keys():
        leverage_info = results[symbol]['leverage_info']
        leverage_tiers = leverage_info.get('tiers', [])
        
        # Get bid and ask USD for the specified level
        bid_data = p50_df[(p50_df['symbol'] == symbol) & 
                         (p50_df['side'] == 'bid') & 
                         (p50_df['level'] == orderbook_level)]
        
        ask_data = p50_df[(p50_df['symbol'] == symbol) & 
                         (p50_df['side'] == 'ask') & 
                         (p50_df['level'] == orderbook_level)]
        
        bid_usd = bid_data['usd'].iloc[0] if len(bid_data) > 0 and not pd.isna(bid_data['usd'].iloc[0]) else 0.0
        ask_usd = ask_data['usd'].iloc[0] if len(ask_data) > 0 and not pd.isna(ask_data['usd'].iloc[0]) else 0.0
        
        # Calculate average
        avg_usd = (bid_usd + ask_usd) / 2 if (bid_usd > 0 or ask_usd > 0) else 0.0
        
        # Get effective leverage for each USD amount
        effective_lev_bid = get_effective_leverage_for_usd_amount(bid_usd, leverage_tiers)
        effective_lev_ask = get_effective_leverage_for_usd_amount(ask_usd, leverage_tiers)  
        effective_lev_avg = get_effective_leverage_for_usd_amount(avg_usd, leverage_tiers)
        
        leverage_usd_data[symbol] = {
            'bid_usd': bid_usd,
            'ask_usd': ask_usd,
            'avg_usd': avg_usd,
            'effective_leverage_bid': effective_lev_bid,
            'effective_leverage_ask': effective_lev_ask,
            'effective_leverage_avg': effective_lev_avg
        }
    
    return leverage_usd_data

def create_leverage_vs_usd_chart(leverage_usd_data: Dict, output_filename: str = CHART_FILENAME, 
                                 chart_type: str = 'avg') -> None:
    """
    Create a column chart: Leverage (X) vs USD Quantity (Y)
    
    Args:
        leverage_usd_data: Data from extract_median_usd_from_p50
        output_filename: Output chart filename
        chart_type: 'bid', 'ask', or 'avg' to determine which data to use
    """
    # Prepare data
    chart_data = []
    
    for symbol, data in leverage_usd_data.items():
        if chart_type == 'bid':
            leverage = data['effective_leverage_bid']
            usd_amount = data['bid_usd']
        elif chart_type == 'ask':
            leverage = data['effective_leverage_ask'] 
            usd_amount = data['ask_usd']
        else:  # avg
            leverage = data['effective_leverage_avg']
            usd_amount = data['avg_usd']
        
        if leverage > 0 and usd_amount > 0:
            chart_data.append({
                'symbol': symbol,
                'leverage': leverage,
                'usd_amount': usd_amount
            })
    
    if not chart_data:
        print("Warning: No data available for leverage vs USD chart")
        return
    
    # Convert to DataFrame
    df = pd.DataFrame(chart_data)
    
    # Create the chart
    plt.figure(figsize=(12, 8))
    
    # Group by leverage for better visualization
    leverage_groups = df.groupby('leverage')
    
    # Create colors for each symbol
    colors = plt.cm.Set3(np.linspace(0, 1, len(df)))
    
    bar_width = 0.8
    positions = []
    labels = []
    values = []
    symbol_labels = []
    
    for i, (leverage, group) in enumerate(leverage_groups):
        symbols = group['symbol'].tolist()
        usd_amounts = group['usd_amount'].tolist()
        
        if len(symbols) == 1:
            # Single bar
            positions.append(leverage)
            values.append(usd_amounts[0])
            symbol_labels.append(symbols[0])
        else:
            # Multiple bars - group them
            n_bars = len(symbols)
            sub_width = bar_width / n_bars
            for j, (symbol, usd_amount) in enumerate(zip(symbols, usd_amounts)):
                pos = leverage + (j - (n_bars-1)/2) * sub_width * 0.3
                positions.append(pos)
                values.append(usd_amount)
                symbol_labels.append(symbol)
    
    # Calculate appropriate bar width based on maximum symbols per leverage level
    max_symbols_per_leverage = df.groupby('leverage').size().max() if len(df) > 0 else 1
    adjusted_bar_width = bar_width / max(1, max_symbols_per_leverage)
    
    # Create bars
    bars = plt.bar(positions, values, width=adjusted_bar_width,
                   color=colors[:len(positions)], alpha=0.8, edgecolor='black', linewidth=0.5)
    
    # Add symbol labels on bars
    for bar, symbol in zip(bars, symbol_labels):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                symbol, ha='center', va='bottom', fontweight='bold', fontsize=10)
    
    # Formatting
    plt.xlabel('Effective Leverage (x)', fontsize=12, fontweight='bold')
    plt.ylabel('Median USD Quantity (p50)', fontsize=12, fontweight='bold')
    chart_type_label = chart_type.upper() if chart_type != 'avg' else 'AVERAGE'
    plt.title(f'Effective Leverage vs Median USD Quantity ({chart_type_label})\nOrderbook Level 1 - Percentile 50', 
              fontsize=14, fontweight='bold', pad=20)
    
    # Format Y-axis to show USD amounts nicely
    ax = plt.gca()
    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}' if x >= 1000 else f'${x:.0f}'))
    
    # Set X-axis to show integer leverage values
    leverage_values = sorted(df['leverage'].unique())
    plt.xticks(leverage_values, [f'{int(lev)}x' for lev in leverage_values])
    
    # Grid for better readability
    plt.grid(True, alpha=0.3, axis='y')
    
    # Tight layout
    plt.tight_layout()
    
    # Save chart
    plt.savefig(output_filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Print summary
    print(f"\n=== LEVERAGE vs USD CHART SUMMARY ({chart_type_label}) ===")
    for leverage in sorted(leverage_values):
        symbols_at_lev = df[df['leverage'] == leverage]['symbol'].tolist()
        amounts_at_lev = df[df['leverage'] == leverage]['usd_amount'].tolist()
        print(f"{int(leverage)}x: {', '.join([f'{s} (${a:,.0f})' for s, a in zip(symbols_at_lev, amounts_at_lev)])}")

def extract_time_series_levels(results: Dict, max_levels: int = 5) -> Dict[str, Dict]:
    """
    Extract time series data for orderbook levels for each symbol.
    
    Args:
        results: Analysis results from analyze_csv
        max_levels: Maximum number of orderbook levels to extract (default: 5)
    
    Returns:
        Dict[symbol] -> {
            'timestamps': List[datetime],
            'bid_levels': Dict[level] -> List[prices],
            'ask_levels': Dict[level] -> List[prices],
            'base_prices': List[float]
        }
    """
    time_series_data = {}
    
    for symbol, blob in results.items():
        sdf = blob['df'].copy()
        
        # Sort by timestamp if available
        if 'ts' in sdf.columns:
            sdf = sdf.sort_values('ts')
        
        # Initialize data structure
        data = {
            'timestamps': [],
            'bid_levels': {i: [] for i in range(1, max_levels + 1)},
            'ask_levels': {i: [] for i in range(1, max_levels + 1)},
            'base_prices': []
        }
        
        # Process each row (timestamp)
        for _, row in sdf.iterrows():
            # Store timestamp
            if 'ts' in row and not pd.isna(row['ts']):
                data['timestamps'].append(row['ts'])
            else:
                # If no timestamp, create a sequential one
                data['timestamps'].append(pd.Timestamp.now() + pd.Timedelta(seconds=len(data['timestamps']) * 300))
            
            # Store base price
            base_price = _normalize_number(row['base_price']) if not pd.isna(row['base_price']) else np.nan
            data['base_prices'].append(base_price)
            
            # Initialize levels with NaN for this timestamp
            for level in range(1, max_levels + 1):
                data['bid_levels'][level].append(np.nan)
                data['ask_levels'][level].append(np.nan)
            
            # Process bids
            if 'bids' in row and not pd.isna(row['bids']):
                try:
                    bid_levels = parse_orderbook_blob(row['bids'])
                    for i, level in enumerate(bid_levels[:max_levels], start=1):
                        px = float(level['px'])
                        data['bid_levels'][i][-1] = px
                except Exception as e:
                    pass
            
            # Process asks
            if 'asks' in row and not pd.isna(row['asks']):
                try:
                    ask_levels = parse_orderbook_blob(row['asks'])
                    for i, level in enumerate(ask_levels[:max_levels], start=1):
                        px = float(level['px'])
                        data['ask_levels'][i][-1] = px
                except Exception as e:
                    pass
        
        time_series_data[symbol] = data
    
    return time_series_data

def create_token_levels_chart(time_series_data: Dict[str, Dict], symbol: str, 
                            output_filename: Optional[str] = None, max_levels: int = 5) -> None:
    """
    Create time series chart showing token price levels over time.
    
    Args:
        time_series_data: Time series data from extract_time_series_levels
        symbol: Symbol to create chart for
        output_filename: Output filename (default: token_levels_over_time_{symbol}.png)
        max_levels: Maximum number of levels to plot
    """
    if symbol not in time_series_data:
        print(f"Warning: No time series data available for {symbol}")
        return
    
    if not output_filename:
        output_filename = f"token_levels_over_time_{symbol}.png"
    
    data = time_series_data[symbol]
    timestamps = data['timestamps']
    
    if not timestamps:
        print(f"Warning: No timestamp data for {symbol}")
        return
    
    plt.figure(figsize=(15, 10))
    
    # Convert timestamps to pandas datetime for better plotting
    timestamps = pd.to_datetime(timestamps)
    
    # Plot base price as reference
    base_prices = data['base_prices']
    plt.plot(timestamps, base_prices, label='Base Price', color='black', linewidth=2, alpha=0.8)
    
    # Colors for different levels
    bid_colors = plt.cm.Blues(np.linspace(0.4, 1, max_levels))
    ask_colors = plt.cm.Reds(np.linspace(0.4, 1, max_levels))
    
    # Plot bid levels
    for level in range(1, max_levels + 1):
        bid_prices = data['bid_levels'][level]
        if any(not np.isnan(p) for p in bid_prices):
            # Convert NaN to None for proper line plotting
            bid_prices_clean = [p if not np.isnan(p) else None for p in bid_prices]
            plt.plot(timestamps, bid_prices_clean, 
                    label=f'Bid L{level}', color=bid_colors[level-1], 
                    alpha=0.7, linewidth=1.5, linestyle='-')
    
    # Plot ask levels
    for level in range(1, max_levels + 1):
        ask_prices = data['ask_levels'][level]
        if any(not np.isnan(p) for p in ask_prices):
            # Convert NaN to None for proper line plotting
            ask_prices_clean = [p if not np.isnan(p) else None for p in ask_prices]
            plt.plot(timestamps, ask_prices_clean, 
                    label=f'Ask L{level}', color=ask_colors[level-1], 
                    alpha=0.7, linewidth=1.5, linestyle='--')
    
    plt.xlabel('Time', fontsize=12, fontweight='bold')
    plt.ylabel('Price (Token Units)', fontsize=12, fontweight='bold')
    plt.title(f'{symbol} - Price Levels Over Time (Token Units)', fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    
    # Format x-axis for time display
    ax = plt.gca()
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))
    ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    
    plt.savefig(output_filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Created token levels chart: {output_filename}")

def create_usd_levels_chart(time_series_data: Dict[str, Dict], symbol: str,
                          output_filename: Optional[str] = None, max_levels: int = 5) -> None:
    """
    Create time series chart showing USD price levels over time.
    
    Args:
        time_series_data: Time series data from extract_time_series_levels  
        symbol: Symbol to create chart for
        output_filename: Output filename (default: usd_levels_over_time_{symbol}.png)
        max_levels: Maximum number of levels to plot
    """
    if symbol not in time_series_data:
        print(f"Warning: No time series data available for {symbol}")
        return
    
    if not output_filename:
        output_filename = f"usd_levels_over_time_{symbol}.png"
    
    data = time_series_data[symbol]
    timestamps = data['timestamps']
    
    if not timestamps:
        print(f"Warning: No timestamp data for {symbol}")
        return
    
    plt.figure(figsize=(15, 10))
    
    # Convert timestamps to pandas datetime for better plotting
    timestamps = pd.to_datetime(timestamps)
    
    # Plot base price as reference
    base_prices = data['base_prices']
    plt.plot(timestamps, base_prices, label='Base Price', color='black', linewidth=2, alpha=0.8)
    
    # Colors for different levels
    bid_colors = plt.cm.Blues(np.linspace(0.4, 1, max_levels))
    ask_colors = plt.cm.Reds(np.linspace(0.4, 1, max_levels))
    
    # Plot bid levels
    for level in range(1, max_levels + 1):
        bid_prices = data['bid_levels'][level]
        if any(not np.isnan(p) for p in bid_prices):
            # Convert NaN to None for proper line plotting
            bid_prices_clean = [p if not np.isnan(p) else None for p in bid_prices]
            plt.plot(timestamps, bid_prices_clean, 
                    label=f'Bid L{level}', color=bid_colors[level-1], 
                    alpha=0.7, linewidth=1.5, linestyle='-')
    
    # Plot ask levels  
    for level in range(1, max_levels + 1):
        ask_prices = data['ask_levels'][level]
        if any(not np.isnan(p) for p in ask_prices):
            # Convert NaN to None for proper line plotting
            ask_prices_clean = [p if not np.isnan(p) else None for p in ask_prices]
            plt.plot(timestamps, ask_prices_clean, 
                    label=f'Ask L{level}', color=ask_colors[level-1], 
                    alpha=0.7, linewidth=1.5, linestyle='--')
    
    plt.xlabel('Time', fontsize=12, fontweight='bold')
    plt.ylabel('Price (USD)', fontsize=12, fontweight='bold')  
    plt.title(f'{symbol} - Price Levels Over Time (USD)', fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    
    # Format Y-axis as USD
    ax = plt.gca()
    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.2f}'))
    
    # Format x-axis for time display
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))
    ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    
    plt.savefig(output_filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Created USD levels chart: {output_filename}")

def analyze_csv(csv_path: str, symbol_filters: Optional[list] = None, sep: str = ',') -> Dict:
    try:
        df = pd.read_csv(csv_path, sep=sep, engine='python')
    except Exception as e:
        raise SystemExit(f"Error reading CSV: {e}")

    validate_required_columns(df.columns.tolist(), set(REQUIRED_CSV_COLUMNS))

    if symbol_filters:
        df = df[df['symbol'].isin(symbol_filters)]
        if len(df) == 0:
            raise SystemExit(f"No data found for symbols: {symbol_filters}")
        found_symbols = df['symbol'].unique()
        missing_symbols = set(symbol_filters) - set(found_symbols)
        if missing_symbols:
            print(f"Warning: Symbols not found in data: {missing_symbols}")

    if 'ts' in df.columns:
        df['ts'] = pd.to_datetime(df['ts'], format='mixed')
        df = df.sort_values('ts')

    results = {}
    for symbol in df['symbol'].unique():
        sdf = df[df['symbol'] == symbol].copy()
        prices = sdf['base_price'].apply(_normalize_number)
        prices = pd.Series(prices, dtype=float)  # Convert to float Series
        
        if 'ts' in sdf.columns:
            prices.index = sdf['ts']

        log_returns = calculate_log_returns(prices)
        vol_metrics = calculate_volatility_metrics(log_returns)
        leverage_info = fetch_leverage_info(symbol)
        
        if leverage_info['status'] == 'error':
            print(f"Warning: Could not get leverage for {symbol}: {leverage_info['error']}")

        results[symbol] = {
            'df': sdf, 'volatility_metrics': vol_metrics, 'leverage_info': leverage_info,
            'price_stats': {
                'min_price': prices.min(), 'max_price': prices.max(), 'mean_price': prices.mean(),
                'latest_price': prices.iloc[-1],
                'price_range_pct': ((prices.max() - prices.min()) / prices.mean()) * 100 if prices.mean() else 0.0,
            },
            'data_quality': {
                'total_observations': len(prices), 'missing_values': prices.isna().sum(),
                'time_span_hours': float((prices.index[-1] - prices.index[0]).total_seconds() / 3600.0) if len(prices) > 1 else 0.0
            }
        }
    return results

def _collect_levels_for_symbol(symbol_df: pd.DataFrame, side: str) -> Dict[int, Dict[str, list]]:
    out = {}
    for _, row in symbol_df.iterrows():
        base_price = _normalize_number(row['base_price'])  # Handle European decimal format
        if side not in row or pd.isna(row[side]):
            continue
        try:
            levels = parse_orderbook_blob(row[side])
        except Exception:
            continue
        for i, lvl in enumerate(levels, start=1):
            try:
                px, sz = float(lvl['px']), float(lvl['sz'])
            except Exception:
                continue
            
            spread_distance = abs((px / base_price - 1.0) * 100.0)
            usd = px * sz
            bucket = out.setdefault(i, {'spread_distance': [], 'qty': [], 'usd': []})
            bucket['spread_distance'].append(spread_distance)
            bucket['qty'].append(sz)
            bucket['usd'].append(usd)
    return out

def _percentile(series: List[float], q: float) -> float:
    return float(pd.Series(series).quantile(q)) if series else float('nan')

def _mean(series: List[float]) -> float:
    return float(pd.Series(series).mean()) if series else float('nan')

def build_percentile_depth(results: Dict, percentiles: List[int]) -> Dict[int, pd.DataFrame]:
    percentile_dfs = {}
    
    for perc in percentiles:
        q = perc / 100.0
        rows = []
        
        for symbol, blob in results.items():
            sdf = blob['df']
            for side in ('bids', 'asks'):
                buckets = _collect_levels_for_symbol(sdf, side)
                acc_qty = acc_usd = 0.0
                for lvl in sorted(buckets.keys()):
                    b = buckets[lvl]
                    spread_p, qty_p, usd_p = _percentile(b['spread_distance'], q), _percentile(b['qty'], q), _percentile(b['usd'], q)
                    if not np.isnan(qty_p): acc_qty += qty_p
                    if not np.isnan(usd_p): acc_usd += usd_p
                    rows.append({
                        'symbol': symbol, 'side': 'bid' if side == 'bids' else 'ask', 'level': lvl,
                        'spread_distance': round(spread_p, 3) if not np.isnan(spread_p) else np.nan,
                        'qty': round(qty_p, 6) if not np.isnan(qty_p) else np.nan,
                        'usd': round(usd_p, 2) if not np.isnan(usd_p) else np.nan,
                        'acc_qty': round(acc_qty, 6) if not np.isnan(acc_qty) else np.nan,
                        'acc_usd': round(acc_usd, 2) if not np.isnan(acc_usd) else np.nan,
                        'obs_count': len(b['qty'])
                    })
        
        df = pd.DataFrame(rows)
        if not df.empty:
            df = df.sort_values(['symbol', 'side', 'level']).reset_index(drop=True)
        percentile_dfs[perc] = df
    
    return percentile_dfs

def build_mean_depth(results: Dict) -> pd.DataFrame:
    """
    Build mean depth data similar to percentile depth but using means instead of percentiles.
    
    Args:
        results: Analysis results from analyze_csv
    
    Returns:
        DataFrame with mean depth data
    """
    rows = []
    
    for symbol, blob in results.items():
        sdf = blob['df']
        for side in ('bids', 'asks'):
            buckets = _collect_levels_for_symbol(sdf, side)
            acc_qty = acc_usd = 0.0
            for lvl in sorted(buckets.keys()):
                b = buckets[lvl]
                spread_m, qty_m, usd_m = _mean(b['spread_distance']), _mean(b['qty']), _mean(b['usd'])
                if not np.isnan(qty_m): acc_qty += qty_m
                if not np.isnan(usd_m): acc_usd += usd_m
                rows.append({
                    'symbol': symbol, 'side': 'bid' if side == 'bids' else 'ask', 'level': lvl,
                    'spread_distance': round(spread_m, 3) if not np.isnan(spread_m) else np.nan,
                    'qty': round(qty_m, 6) if not np.isnan(qty_m) else np.nan,
                    'usd': round(usd_m, 2) if not np.isnan(usd_m) else np.nan,
                    'acc_qty': round(acc_qty, 6) if not np.isnan(acc_qty) else np.nan,
                    'acc_usd': round(acc_usd, 2) if not np.isnan(acc_usd) else np.nan,
                    'obs_count': len(b['qty'])
                })
    
    df = pd.DataFrame(rows)
    if not df.empty:
        df = df.sort_values(['symbol', 'side', 'level']).reset_index(drop=True)
    
    return df

def save_percentile_depth_csvs(results: Dict, percentiles: List[int]) -> List[Path]:
    percentile_dfs = build_percentile_depth(results, percentiles)
    output_paths = []
    
    for perc, df in percentile_dfs.items():
        output_csv = f"orderbook_levels_p{perc:02d}.csv"
        out_path = Path(output_csv)
        
        df_renamed = df.rename(columns={
            'spread_distance': f'spread_distance_p{perc:02d}', 'qty': f'qty_p{perc:02d}',
            'usd': f'usd_p{perc:02d}', 'acc_qty': f'acc_qty_p{perc:02d}', 'acc_usd': f'acc_usd_p{perc:02d}',
        })
        
        df_renamed.to_csv(out_path, index=False)
        output_paths.append(out_path)
    
    return output_paths

def _fmt_price(p: float) -> str:
    if p is None or (isinstance(p, float) and np.isnan(p)): return "N/A"
    if p >= 1000: return f"{p:,.0f}"
    if p >= 1: return f"{p:.2f}"
    return f"{p:.4f}"

def _fmt_volume(v: float) -> str:
    """Format volume in USD with appropriate scaling"""
    if v is None or (isinstance(v, float) and np.isnan(v)): return "N/A"
    if v >= 1000000: return f"${v/1000000:.1f}M"
    if v >= 1000: return f"${v/1000:.0f}K"
    return f"${v:.0f}"

def calculate_max_investment_volume(symbol_df: pd.DataFrame, target_price: float, side: str) -> float:
    """
    Calculate maximum USD investment volume needed to reach target price by absorbing orderbook levels.
    
    Args:
        symbol_df: DataFrame with orderbook data for the symbol
        target_price: Target price to reach
        side: 'buy' for absorbing asks, 'sell' for absorbing bids
    
    Returns:
        Total USD volume needed to reach target price
    """
    total_volume_usd = 0.0
    
    # Get the most recent orderbook snapshot (last row)
    if len(symbol_df) == 0:
        return 0.0
    
    last_row = symbol_df.iloc[-1]
    base_price = _normalize_number(last_row['base_price'])
    
    if pd.isna(base_price) or base_price <= 0:
        return 0.0
    
    # Parse orderbook levels
    orderbook_side = 'asks' if side == 'buy' else 'bids'
    
    if orderbook_side not in last_row or pd.isna(last_row[orderbook_side]):
        return 0.0
    
    try:
        levels = parse_orderbook_blob(last_row[orderbook_side])
    except Exception:
        return 0.0
    
    if not levels:
        return 0.0
    
    # Sort levels appropriately
    if side == 'buy':
        # For buying, we want asks sorted by price (ascending)
        levels = sorted(levels, key=lambda x: float(x['px']))
        # Check if target is reachable
        if target_price < float(levels[0]['px']):
            return 0.0
    else:
        # For selling, we want bids sorted by price (descending)
        levels = sorted(levels, key=lambda x: float(x['px']), reverse=True)
        # Check if target is reachable
        if target_price > float(levels[0]['px']):
            return 0.0
    
    # Accumulate volume level by level
    prev_price = None
    
    for i, level in enumerate(levels):
        try:
            level_price = float(level['px'])
            level_size = float(level['sz'])
        except (ValueError, KeyError):
            continue
        
        if side == 'buy':
            if level_price <= target_price:
                # Absorb entire level
                total_volume_usd += level_price * level_size
                prev_price = level_price
            else:
                # Target price falls between prev_price and level_price
                if prev_price is not None and i > 0:
                    # Linear interpolation
                    price_range = level_price - prev_price
                    target_offset = target_price - prev_price
                    interpolation_factor = target_offset / price_range if price_range > 0 else 0
                    interpolated_size = level_size * interpolation_factor
                    total_volume_usd += target_price * interpolated_size
                break
        else:  # sell
            if level_price >= target_price:
                # Absorb entire level
                total_volume_usd += level_price * level_size
                prev_price = level_price
            else:
                # Target price falls between level_price and prev_price
                if prev_price is not None and i > 0:
                    # Linear interpolation
                    price_range = prev_price - level_price
                    target_offset = prev_price - target_price
                    interpolation_factor = target_offset / price_range if price_range > 0 else 0
                    interpolated_size = level_size * interpolation_factor
                    total_volume_usd += target_price * interpolated_size
                break
    
    return total_volume_usd

def create_new_percentile_tables(results: Dict, percentiles: List[int], ws, start_row: int) -> int:
    """
    Create new percentile tables with all percentiles side by side, plus mean columns at the end.
    
    Args:
        results: Analysis results
        percentiles: List of percentiles to analyze
        ws: Excel worksheet
        start_row: Starting row number
    
    Returns:
        Next available row number
    """
    # Get percentile data and mean data
    percentile_dfs = build_percentile_depth(results, percentiles)
    mean_df = build_mean_depth(results)
    
    # Get symbols and determine max levels
    symbols = list(results.keys())
    max_levels = {}
    
    # Check percentile data for max levels
    for perc in percentiles:
        pct_df = percentile_dfs[perc]
        for side in ['ask', 'bid']:
            for symbol in symbols:
                symbol_data = pct_df[(pct_df['symbol'] == symbol) & (pct_df['side'] == side)]
                if not symbol_data.empty:
                    max_level = symbol_data['level'].max()
                    max_levels[symbol] = max(max_levels.get(symbol, 0), max_level)
    
    # Check mean data for max levels
    for side in ['ask', 'bid']:
        for symbol in symbols:
            symbol_data = mean_df[(mean_df['symbol'] == symbol) & (mean_df['side'] == side)]
            if not symbol_data.empty:
                max_level = symbol_data['level'].max()
                max_levels[symbol] = max(max_levels.get(symbol, 0), max_level)
    
    # Determine overall max level across all symbols
    overall_max_level = max(max_levels.values()) if max_levels else 5
    
    # Define styles
    header_font = Font(bold=True, size=12, color="FFFFFF")
    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    mean_header_fill = PatternFill(start_color="FF6B35", end_color="FF6B35", fill_type="solid")  # Orange for mean columns
    title_font = Font(bold=True, size=14, color="000000")
    center = Alignment(horizontal="center", vertical="center")
    
    current_row = start_row
    
    # BIG TABLE A: Token Quantities (All Percentiles Side by Side + Mean Columns)
    ws.cell(row=current_row, column=1, value="5. Token Quantities - All Percentiles + Means (Accumulated Quantities)").font = title_font
    current_row += 2
    
    # Calculate header structure for all percentiles + means
    col = 2
    header_positions = []
    
    # Percentile columns
    for perc in sorted(percentiles):
        for side in ['Ask', 'Bid']:
            start_col = col
            # Create merged header for this side-percentile combination
            for level in range(1, overall_max_level + 1):
                header_positions.append((current_row + 1, col, f'N{level}-{perc}%'))
                col += 1
            end_col = col - 1
            
            # Merge cells for side-percentile header
            if start_col <= end_col:
                ws.merge_cells(start_row=current_row, start_column=start_col, end_row=current_row, end_column=end_col)
                merged_cell = ws.cell(row=current_row, column=start_col, value=f"{side} (p{perc:02d})")
                merged_cell.font = header_font
                merged_cell.fill = header_fill
                merged_cell.alignment = center
    
    # Mean columns
    for side in ['Ask', 'Bid']:
        start_col = col
        # Create merged header for this side-mean combination
        for level in range(1, overall_max_level + 1):
            header_positions.append((current_row + 1, col, f'N{level}-mean'))
            col += 1
        end_col = col - 1
        
        # Merge cells for side-mean header
        if start_col <= end_col:
            ws.merge_cells(start_row=current_row, start_column=start_col, end_row=current_row, end_column=end_col)
            merged_cell = ws.cell(row=current_row, column=start_col, value=f"{side} (mean)")
            merged_cell.font = header_font
            merged_cell.fill = mean_header_fill
            merged_cell.alignment = center
    
    current_row += 1
    
    # Second header row with level labels
    for row_num, col_num, label in header_positions:
        cell = ws.cell(row=row_num, column=col_num, value=label)
        cell.font = header_font
        if 'mean' in label:
            cell.fill = mean_header_fill
        else:
            cell.fill = header_fill
        cell.alignment = center
    
    current_row += 1
    
    # Data rows for token quantities (all percentiles + means)
    for symbol in symbols:
        ws.cell(row=current_row, column=1, value=symbol).font = Font(bold=True)
        ws.cell(row=current_row, column=1).alignment = center
        
        col = 2
        # Percentile data
        for perc in sorted(percentiles):
            pct_df = percentile_dfs[perc]
            for side in ['ask', 'bid']:
                for level in range(1, overall_max_level + 1):
                    # Get data for this combination
                    data_row = pct_df[(pct_df['symbol'] == symbol) & 
                                     (pct_df['side'] == side) & 
                                     (pct_df['level'] == level)]
                    
                    if len(data_row) > 0:
                        value = data_row['acc_qty'].iloc[0]
                        display_val = round(value, 6) if not pd.isna(value) else "N/A"
                    else:
                        display_val = "N/A"
                    
                    ws.cell(row=current_row, column=col, value=display_val).alignment = center
                    col += 1
        
        # Mean data
        for side in ['ask', 'bid']:
            for level in range(1, overall_max_level + 1):
                # Get mean data for this combination
                data_row = mean_df[(mean_df['symbol'] == symbol) & 
                                  (mean_df['side'] == side) & 
                                  (mean_df['level'] == level)]
                
                if len(data_row) > 0:
                    value = data_row['acc_qty'].iloc[0]
                    display_val = round(value, 6) if not pd.isna(value) else "N/A"
                else:
                    display_val = "N/A"
                
                ws.cell(row=current_row, column=col, value=display_val).alignment = center
                col += 1
        
        current_row += 1
    
    current_row += 3
    
    # BIG TABLE B: USD Values (All Percentiles Side by Side + Mean Columns)
    ws.cell(row=current_row, column=1, value="6. USD Values - All Percentiles + Means (Accumulated USD)").font = title_font
    current_row += 2
    
    # Reset column counter and header positions
    col = 2
    header_positions = []
    
    # Percentile columns
    for perc in sorted(percentiles):
        for side in ['Ask', 'Bid']:
            start_col = col
            # Create merged header for this side-percentile combination
            for level in range(1, overall_max_level + 1):
                header_positions.append((current_row + 1, col, f'N{level}-{perc}%'))
                col += 1
            end_col = col - 1
            
            # Merge cells for side-percentile header
            if start_col <= end_col:
                ws.merge_cells(start_row=current_row, start_column=start_col, end_row=current_row, end_column=end_col)
                merged_cell = ws.cell(row=current_row, column=start_col, value=f"{side} (p{perc:02d})")
                merged_cell.font = header_font
                merged_cell.fill = header_fill
                merged_cell.alignment = center
    
    # Mean columns
    for side in ['Ask', 'Bid']:
        start_col = col
        # Create merged header for this side-mean combination
        for level in range(1, overall_max_level + 1):
            header_positions.append((current_row + 1, col, f'N{level}-mean'))
            col += 1
        end_col = col - 1
        
        # Merge cells for side-mean header
        if start_col <= end_col:
            ws.merge_cells(start_row=current_row, start_column=start_col, end_row=current_row, end_column=end_col)
            merged_cell = ws.cell(row=current_row, column=start_col, value=f"{side} (mean)")
            merged_cell.font = header_font
            merged_cell.fill = mean_header_fill
            merged_cell.alignment = center
    
    current_row += 1
    
    # Second header row with level labels
    for row_num, col_num, label in header_positions:
        cell = ws.cell(row=row_num, column=col_num, value=label)
        cell.font = header_font
        if 'mean' in label:
            cell.fill = mean_header_fill
        else:
            cell.fill = header_fill
        cell.alignment = center
    
    current_row += 1
    
    # Data rows for USD values (all percentiles + means)
    for symbol in symbols:
        ws.cell(row=current_row, column=1, value=symbol).font = Font(bold=True)
        ws.cell(row=current_row, column=1).alignment = center
        
        col = 2
        # Percentile data
        for perc in sorted(percentiles):
            pct_df = percentile_dfs[perc]
            for side in ['ask', 'bid']:
                for level in range(1, overall_max_level + 1):
                    # Get data for this combination
                    data_row = pct_df[(pct_df['symbol'] == symbol) & 
                                     (pct_df['side'] == side) & 
                                     (pct_df['level'] == level)]
                    
                    if len(data_row) > 0:
                        value = data_row['acc_usd'].iloc[0]
                        display_val = f"${round(value, 2)}" if not pd.isna(value) else "N/A"
                    else:
                        display_val = "N/A"
                    
                    ws.cell(row=current_row, column=col, value=display_val).alignment = center
                    col += 1
        
        # Mean data
        for side in ['ask', 'bid']:
            for level in range(1, overall_max_level + 1):
                # Get mean data for this combination
                data_row = mean_df[(mean_df['symbol'] == symbol) & 
                                  (mean_df['side'] == side) & 
                                  (mean_df['level'] == level)]
                
                if len(data_row) > 0:
                    value = data_row['acc_usd'].iloc[0]
                    display_val = f"${round(value, 2)}" if not pd.isna(value) else "N/A"
                else:
                    display_val = "N/A"
                
                ws.cell(row=current_row, column=col, value=display_val).alignment = center
                col += 1
        
        current_row += 1
    
    current_row += 3
    
    # Table C: Summary (updated to include mean averages)
    ws.cell(row=current_row, column=1, value="7. Summary - Average Spread Distances (Percentiles + Means)").font = title_font
    current_row += 2
    
    # Summary table headers
    headers = ["Token", "Level", "Avg Delta Ask % (Percentiles)", "Avg Delta Bid % (Percentiles)", "Avg Delta Ask % (Mean)", "Avg Delta Bid % (Mean)"]
    for c, hdr in enumerate(headers, start=1):
        cell = ws.cell(row=current_row, column=c, value=hdr)
        cell.font = header_font
        if "Mean" in hdr:
            cell.fill = mean_header_fill
        else:
            cell.fill = header_fill
        cell.alignment = center
    current_row += 1
    
    # Summary data
    for symbol in symbols:
        for level in range(1, max_levels.get(symbol, overall_max_level) + 1):
            # Calculate average spread distances across all percentiles
            ask_spreads_pct = []
            bid_spreads_pct = []
            
            for perc in percentiles:
                pct_df = percentile_dfs[perc]
                
                ask_data = pct_df[(pct_df['symbol'] == symbol) & 
                                 (pct_df['side'] == 'ask') & 
                                 (pct_df['level'] == level)]
                if len(ask_data) > 0 and not pd.isna(ask_data['spread_distance'].iloc[0]):
                    ask_spreads_pct.append(ask_data['spread_distance'].iloc[0])
                
                bid_data = pct_df[(pct_df['symbol'] == symbol) & 
                                 (pct_df['side'] == 'bid') & 
                                 (pct_df['level'] == level)]
                if len(bid_data) > 0 and not pd.isna(bid_data['spread_distance'].iloc[0]):
                    bid_spreads_pct.append(bid_data['spread_distance'].iloc[0])
            
            # Get mean spread distances
            ask_data_mean = mean_df[(mean_df['symbol'] == symbol) & 
                                   (mean_df['side'] == 'ask') & 
                                   (mean_df['level'] == level)]
            bid_data_mean = mean_df[(mean_df['symbol'] == symbol) & 
                                   (mean_df['side'] == 'bid') & 
                                   (mean_df['level'] == level)]
            
            ask_spread_mean = ask_data_mean['spread_distance'].iloc[0] if len(ask_data_mean) > 0 and not pd.isna(ask_data_mean['spread_distance'].iloc[0]) else np.nan
            bid_spread_mean = bid_data_mean['spread_distance'].iloc[0] if len(bid_data_mean) > 0 and not pd.isna(bid_data_mean['spread_distance'].iloc[0]) else np.nan
            
            avg_ask_pct = np.mean(ask_spreads_pct) if ask_spreads_pct else np.nan
            avg_bid_pct = np.mean(bid_spreads_pct) if bid_spreads_pct else np.nan
            
            # Write summary row
            ws.cell(row=current_row, column=1, value=symbol).alignment = center
            ws.cell(row=current_row, column=2, value=f"N{level}").alignment = center
            ws.cell(row=current_row, column=3, value=round(avg_ask_pct, 3) if not np.isnan(avg_ask_pct) else "N/A").alignment = center
            ws.cell(row=current_row, column=4, value=round(avg_bid_pct, 3) if not np.isnan(avg_bid_pct) else "N/A").alignment = center
            ws.cell(row=current_row, column=5, value=round(ask_spread_mean, 3) if not np.isnan(ask_spread_mean) else "N/A").alignment = center
            ws.cell(row=current_row, column=6, value=round(bid_spread_mean, 3) if not np.isnan(bid_spread_mean) else "N/A").alignment = center
            
            current_row += 1
    
    return current_row

def create_excel_tables(results: Dict, percentiles: List[int], output_filename: str = EXCEL_FILENAME) -> None:
    workbook = openpyxl.Workbook()
    ws = workbook.active
    ws.title = "Trading Analysis"

    header_font = Font(bold=True, size=12, color="FFFFFF")
    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    title_font = Font(bold=True, size=14, color="000000")
    center = Alignment(horizontal="center", vertical="center")
    leverage_fill = PatternFill(start_color="FFF8DC", end_color="FFF8DC", fill_type="solid")

    symbols = list(results.keys())
    time_labels = ["3min", "5min", "10min", "30min", "60min", "90min", "1day", "1year"]
    vol_keys = ['3min_vol','5min_vol','10min_vol','30min_vol','60min_vol','90min_vol','1day_vol','1year_vol']

    row = 1
    ws.cell(row=row, column=1, value="NOTE: All percentage values are numbers without % symbol for Excel editing").font = Font(italic=True, size=10, color="666666")
    row += 2

    # Table 1: Volatility Analysis (Transposed with Leverage Info)
    ws.cell(row=row, column=1, value="1. Volatility Analysis - Delta Percentages + Leverage Info").font = title_font
    row += 2

    # Headers: Symbol + time periods + leverage columns
    headers = ["Symbol"] + [("1 day" if tl=="1day" else "1 year" if tl=="1year" else tl) for tl in time_labels] + ["Real Max Leverage", "Max Quantity", "Second Leverage"]
    
    for c, hdr in enumerate(headers, start=1):
        cell = ws.cell(row=row, column=c, value=hdr)
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = center
    row += 1

    # Data rows: each symbol with its volatility values + leverage info
    for sym in symbols:
        ws.cell(row=row, column=1, value=sym).font = Font(bold=True)
        ws.cell(row=row, column=1).alignment = center
        
        # Volatility data
        for c, vk in enumerate(vol_keys, start=2):
            v = results[sym]['volatility_metrics'][vk]
            ws.cell(row=row, column=c, value=round(v*100, 3)).alignment = center
        
        # Leverage data
        lev_info = results[sym]['leverage_info']
        tiers = lev_info.get('tiers', [])
        
        # Real Max Leverage
        real_max_lev = lev_info.get('max_leverage', 0)
        ws.cell(row=row, column=len(headers)-2, value=real_max_lev).alignment = center
        
        # Max Quantity and Second Leverage (for multi-tier systems)
        if len(tiers) >= 2:
            # Multi-tier system
            max_quantity = tiers[0][1]  # Upper bound of first tier
            second_leverage = tiers[1][2]  # Leverage of second tier
            
            # Format max quantity
            if max_quantity == float('inf'):
                max_qty_str = "N/A"
            else:
                max_qty_str = f"${max_quantity:,.0f}"
            
            ws.cell(row=row, column=len(headers)-1, value=max_qty_str).alignment = center
            ws.cell(row=row, column=len(headers), value=second_leverage).alignment = center
        else:
            # Single tier system
            ws.cell(row=row, column=len(headers)-1, value="N/A").alignment = center
            ws.cell(row=row, column=len(headers), value="N/A").alignment = center
        
        row += 1

    row += 2

    # Table 2: Buy Investment Volumes 
    ws.cell(row=row, column=1, value="2. Max Investment Volume to Buy (Absorbing Ask Levels)").font = title_font
    row += 2

    # Headers: Symbol + time periods
    headers = ["Symbol"] + [("1 day" if tl=="1day" else "1 year" if tl=="1year" else tl) for tl in time_labels]
    
    for c, hdr in enumerate(headers, start=1):
        cell = ws.cell(row=row, column=c, value=hdr)
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = center
    row += 1

    # Data rows: each symbol with max investment volumes for buy targets
    for sym in symbols:
        ws.cell(row=row, column=1, value=sym).font = Font(bold=True)
        ws.cell(row=row, column=1).alignment = center
        
        symbol_df = results[sym]['df']
        base_price = results[sym]['price_stats']['latest_price']
        
        for c, vk in enumerate(vol_keys, start=2):
            vol = results[sym]['volatility_metrics'][vk]
            target_price = base_price + (vol * base_price)  # Buy target (base + slippage)
            
            max_volume = calculate_max_investment_volume(symbol_df, target_price, 'buy')
            ws.cell(row=row, column=c, value=_fmt_volume(max_volume)).alignment = center
        
        row += 1

    row += 2

    # Table 3: Sell Investment Volumes
    ws.cell(row=row, column=1, value="3. Max Investment Volume to Sell (Absorbing Bid Levels)").font = title_font
    row += 2

    # Headers: Symbol + time periods  
    for c, hdr in enumerate(headers, start=1):
        cell = ws.cell(row=row, column=c, value=hdr)
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = center
    row += 1

    # Data rows: each symbol with max investment volumes for sell targets
    for sym in symbols:
        ws.cell(row=row, column=1, value=sym).font = Font(bold=True)
        ws.cell(row=row, column=1).alignment = center
        
        symbol_df = results[sym]['df']
        base_price = results[sym]['price_stats']['latest_price']
        
        for c, vk in enumerate(vol_keys, start=2):
            vol = results[sym]['volatility_metrics'][vk]
            target_price = base_price - (vol * base_price)  # Sell target (base - slippage)
            
            max_volume = calculate_max_investment_volume(symbol_df, target_price, 'sell')
            ws.cell(row=row, column=c, value=_fmt_volume(max_volume)).alignment = center
        
        row += 1

    row += 2

    # Table 4: Leverage Information
    ws.cell(row=row, column=1, value="4. Leverage Information").font = title_font
    row += 2

    headers = ["Symbol", "Leverage Tiers"]
    for c, hdr in enumerate(headers, start=1):
        cell = ws.cell(row=row, column=c, value=hdr)
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = center
    row += 1

    for sym in symbols:
        lev_info = results[sym]['leverage_info']
        
        ws.cell(row=row, column=1, value=sym).alignment = center
        ws.cell(row=row, column=1).font = Font(bold=True)
        ws.cell(row=row, column=1).fill = leverage_fill
        
        tiers_text = "; ".join([f"{lb}  {lev}" for lb, lev in lev_info['tiers_formatted']]) if lev_info['tiers_formatted'] else "N/A"
        ws.cell(row=row, column=2, value=tiers_text).alignment = Alignment(horizontal="left", vertical="center")
        ws.cell(row=row, column=2).fill = leverage_fill
        
        row += 1

    row += 2

    # New Percentile Tables (replaces the old ones and includes means)
    row = create_new_percentile_tables(results, percentiles, ws, row)

    # Column widths - increased for more columns
    ws.column_dimensions['A'].width = 12
    for c in range(2, len(symbols) + 50):  # More columns due to mean data
        ws.column_dimensions[get_column_letter(c)].width = 12

    workbook.save(output_filename)

def parse_arguments():
    parser = argparse.ArgumentParser(description="Volatility and orderbook analysis with configurable percentiles, leverage chart, and time series charts")
    parser.add_argument('csv_file', help='Path to CSV file with orderbook data')
    parser.add_argument('symbols', nargs='*', help='Optional: specific symbols to analyze')
    parser.add_argument('--percentiles', nargs='+', type=int, default=DEFAULT_PERCENTILES,
                       help=f'Percentiles to calculate (1-99). Default: {DEFAULT_PERCENTILES}')
    parser.add_argument('--sep', default=',', 
                       help='CSV separator/delimiter (default: ",")')
    parser.add_argument('--chart-level', type=int, default=1,
                       help='Orderbook level to use for leverage chart (default: 1)')
    parser.add_argument('--chart-type', choices=['bid', 'ask', 'avg'], default='avg',
                       help='Type of data to use for leverage chart: bid, ask, or avg (default: avg)')
    parser.add_argument('--max-levels', type=int, default=5,
                       help='Maximum number of orderbook levels to plot in time series charts (default: 5)')
    
    args = parser.parse_args()
    
    for p in args.percentiles:
        if not 1 <= p <= 99:
            parser.error(f"Percentile {p} must be between 1 and 99")
    
    args.percentiles = sorted(set(args.percentiles))
    
    # Ensure percentile 50 is included for the chart
    if 50 not in args.percentiles:
        args.percentiles.append(50)
        args.percentiles = sorted(args.percentiles)
        print("Note: Added percentile 50 (required for leverage chart)")
    
    return args

def main():
    args = parse_arguments()
    
    if not Path(args.csv_file).exists():
        raise SystemExit(f"CSV file not found: {args.csv_file}")

    # Check if leverage data file exists
    if not Path(LEVERAGE_DATA_FILE).exists():
        print(f"Warning: Leverage data file not found: {LEVERAGE_DATA_FILE}")
        print(f"To generate leverage data, run: python API.py [symbols...]")
        print("Proceeding with missing leverage data (will show errors for leverage info)...")

    print("Analyzing CSV and loading leverage data...")
    results = analyze_csv(args.csv_file, args.symbols, sep=args.sep)
    
    print("Creating percentile depth tables...")
    csv_paths = save_percentile_depth_csvs(results, args.percentiles)
    
    print("Creating Excel tables...")
    create_excel_tables(results, args.percentiles, output_filename=EXCEL_FILENAME)
    
    print("Extracting median USD data and creating leverage chart...")
    leverage_usd_data = extract_median_usd_from_p50(results, args.percentiles, args.chart_level)
    create_leverage_vs_usd_chart(leverage_usd_data, output_filename=CHART_FILENAME, chart_type=args.chart_type)
    
    print("Extracting time series data and creating level charts...")
    time_series_data = extract_time_series_levels(results, max_levels=args.max_levels)
    
    # Create time series charts for each symbol
    chart_files = []
    for symbol in results.keys():
        token_chart = f"token_levels_over_time_{symbol}.png"
        usd_chart = f"usd_levels_over_time_{symbol}.png"
        
        create_token_levels_chart(time_series_data, symbol, token_chart, max_levels=args.max_levels)
        create_usd_levels_chart(time_series_data, symbol, usd_chart, max_levels=args.max_levels)
        
        chart_files.extend([token_chart, usd_chart])
    
    print("\n=== OUTPUT FILES ===")
    for path in csv_paths:
        print(f"{path}")
    print(f"{EXCEL_FILENAME}")
    print(f"{CHART_FILENAME}")
    
    print("\n=== TIME SERIES CHARTS ===")
    for chart_file in chart_files:
        print(f"{chart_file}")

if __name__ == "__main__":
    main()